{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TMO 2017-2018: practica-toxic.py\n",
    "# Dpto. de C. de la Computación e I.A. (Univ. de Sevilla)\n",
    "#=====================================================================\n",
    "\n",
    "# ********************************************************************\n",
    "# Nombre: Jorge Miguel\n",
    "# Apellidos: García García\n",
    "# ********************************************************************\n",
    "\n",
    "# **************************** IMPORTANTE ****************************\n",
    "# - Recordar escribir el nombre en la cabecera de este fichero.\n",
    "# ********************************************************************\n",
    "\n",
    "# ********************************************************************\n",
    "# HONESTIDAD ACADÉMICA Y COPIAS: la realización de los ejercicios es\n",
    "# un trabajo personal, por lo que deben completarse por cada\n",
    "# estudiante de manera individual.  La discusión y el intercambio de\n",
    "# información de carácter general con los compañeros se permite (e\n",
    "# incluso se recomienda), pero NO AL NIVEL DE CÓDIGO. Igualmente el\n",
    "# remitir código de terceros, obtenido a través de la red o cualquier\n",
    "# otro medio, se considerará plagio.\n",
    "\n",
    "# Cualquier plagio o compartición de código que se detecte significará\n",
    "# automáticamente la calificación de CERO EN LA ASIGNATURA para TODOS\n",
    "# los alumnos involucrados. \n",
    "# ********************************************************************\n",
    "\n",
    "# Se pide crear un modelo de clasificación con alguno de los algoritmos\n",
    "# vistos en clase e implementados en la librería scikit-learn, también se\n",
    "# permite el uso de xgboost y keras (tensorflow). Se valorará\n",
    "# el ajuste de parámetros realizado (aplicando validación cruzada), así como\n",
    "# la transformaciones sobre los datos desarrolladas para mejorar el calidad\n",
    "# (score) del modelo por validación cruzada (un buen ejemplo es el notebook\n",
    "# Left que se puede encontrar en la enseñanza virtual)\n",
    "\n",
    "# El conjunto de datos corresponde a comentarios realizados en wikipedia.\n",
    "# Se pretende determinar si los comentarios tiene un carga negativa o no.\n",
    "# que un anuncio dado tendrá para la comunidad de usuarios de este portal.\n",
    "# Cada comentario puede ser clasificado como tóxico, muy tóxico, obsceno,\n",
    "# insultante, con carga de odio y/o amenazante.\n",
    "\n",
    "#    1. De los texto se pueden obtener atributos como la longitud, número \n",
    "#       de palabras, número de palabras únicas, número de mayúsculas, etc... \n",
    "#       También se pueden aplicar técnicas de vectorización de textos como \n",
    "#       CountVectorizer o TF-IDF entre otras. \n",
    "#    2. Se puede usar diferentes algoritmos: regresión lineal, naive bayes, \n",
    "#       random forest y despues intentar ensamblarlos (obtener la media de las\n",
    "#       predicciones)\n",
    "\n",
    "# Estas son algunas de las tranformaciones que se pueden realizar, pero no las\n",
    "# únicas. Cualquier otra transformación que se lleve a cabo sobre los datos\n",
    "# será tenida en cuenta positivamente.\n",
    "\n",
    "# Se deberá entregar este archivo con las implementaciones realizadas y\n",
    "# comentadas, el archivo de predicciones del mejor modelo encontrado y el score \n",
    "# asociado por cross-validación y el proporcionado por kaggle (enviar a \n",
    "# dsolis@us.es)\n",
    "\n",
    "# Toda la información necesaria y los conjuntos de datos se pueden encontrar en:\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "# *******************************************************************\n",
    "# IMPORTANTE: El plazo de entrega es hasta la finalización de la \n",
    "# competición (20 de Febrero).\n",
    "# ********************************************************************\n",
    "\n",
    "# *******************************************************************\n",
    "# IMPORTANTE: La competición está activa. Debéis subir las predicciones\n",
    "# y que la plataforma os dé el score sobre el test. Podeis usar el nombre\n",
    "# de usuario que considereis oportuno.\n",
    "# ********************************************************************\n",
    "\n",
    "# ********************************************************************\n",
    "# IMPORTANTE: Se pueden consultar y usar los ejemplos de código y \n",
    "# transformaciones encontradas en: \n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels\n",
    "# ********************************************************************\n",
    "\n",
    "# ********************************************************************\n",
    "# IMPORTANTE: Para resolver cualquier duda contactar con David Solís \n",
    "# (dsolis@us.es)\n",
    "# ********************************************************************\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de librerías principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#propias\n",
    "import constants as ct\n",
    "from utils import get_upper_case_prop, get_punctuation_prop, process_text\n",
    "from utils_plot import PlotLosses\n",
    "import string_dists as stds\n",
    "\n",
    "#keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Layer, Lambda\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from functools import reduce\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "#from scipy.misc import imread\n",
    "import scipy.sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "import matplotlib_venn as venn\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "#FeatureEngineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.width = 1000\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "#settingsimread\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos también el array de badwords que hemos preparado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BLACK_LIST = pd.read_csv('arrBad', index_col=None, quotechar=\"'\")\n",
    "#BLACK_LIST = BLACK_LIST.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de los datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col='id')\n",
    "X_train = train['comment_text'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_final_cols = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.OBJECTIVE_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train[ct.OBJECTIVE_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el dataset tiene clases muy desequilibradas, especialmente **threat** y **identity_hate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            9.584448\n",
       "obscene          5.294822\n",
       "insult           4.936361\n",
       "severe_toxic     0.999555\n",
       "identity_hate    0.880486\n",
       "threat           0.299553\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.sum().div(Y_train.shape[0] / 100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-957e0741f098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv('test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto combinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.concat([X_train, X_test])\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Y_train.iloc[:,:].sum()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"# Por clase\")\n",
    "plt.ylabel('# comentarios', fontsize=12)\n",
    "plt.xlabel('Tipos ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las implicaciones entre las diferentes etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implications = pd.DataFrame(np.nan, index=Y_train.columns, columns=Y_train.columns)\n",
    "\n",
    "for col_ant in Y_train.columns:\n",
    "    for col_con in Y_train.columns:\n",
    "        \n",
    "        selection_df = Y_train.loc[Y_train[col_ant] == 1]\n",
    "        n_ant = selection_df.shape[0]\n",
    "        n_con = selection_df[col_con].sum()\n",
    "        implications.loc[col_ant, col_con] = n_con / n_ant\n",
    "        \n",
    "implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de atributos\n",
    "\n",
    " - Número de frases\n",
    " - Número de palabras\n",
    " - Número de palabras (únicas)\n",
    " - Número de letras\n",
    " - Número de signos de puntuacion\n",
    " - Número de mayúsculas\n",
    " - Número de _title case_\n",
    " - Número de stopwords (inglés)\n",
    " - Número de interrogantes\n",
    " - Número de signos de admiración\n",
    " - Número y control de IPs\n",
    " - Número y control de usuarios\n",
    " - Número y control de enlaces\n",
    " - Artículos\n",
    " \n",
    " - Media de lontigud de palabra\n",
    " - Ratio (100) de palabras únicas frente a palabras\n",
    " - Ratio (100) de signos de puntuacion frente a palabras\n",
    " - Ratio (100) de mayúsculas frente a letras (omitimos signos de puntuacion)\n",
    " \n",
    " - _Ratio de distancia a palabras de una bolsa de palabras malsonantes/ofensivas_ (deprecated) habría que pensar otra manera de hacer este cálculo ya que explota debido a la alta combinatoriedad, \"orden 10^10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counting\n",
    "X_full['n_sentences'] = X_full[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "X_full['n_words']=X_full[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "X_full['n_unique_word']=X_full[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "X_full['n_letters']=X_full[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "X_full[\"n_punct\"] =X_full[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "X_full[\"n_upper\"] = X_full[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "X_full[\"n_title\"] = X_full[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "X_full[\"n_stopwords\"] = X_full[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "X_full['n_qmark'] = X_full['comment_text'].apply(lambda x: len(re.findall(\"\\?\",str(x))))\n",
    "X_full['n_exmark'] = X_full['comment_text'].apply(lambda x: len(re.findall(\"!\",str(x))))\n",
    "X_full['ip']=X_full[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\n",
    "X_full['n_ip']=X_full[\"ip\"].apply(lambda x: len(x))\n",
    "X_full['link']=X_full[\"comment_text\"].apply(lambda x: re.findall(\"http://.*com\",str(x)))\n",
    "X_full['n_links']=X_full[\"link\"].apply(lambda x: len(x))\n",
    "X_full['article_id']=X_full[\"comment_text\"].apply(lambda x: re.findall(\"\\d:\\d\\d\\s{0,5}$\",str(x)))\n",
    "X_full['article_id_flag']=X_full.article_id.apply(lambda x: len(x))\n",
    "X_full['username']=X_full[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))\n",
    "X_full['n_usernames']=X_full[\"username\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Measures\n",
    "X_full[\"mean_word_len\"] = X_full[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "X_full['per_wordunique']=X_full['n_unique_word']*100/X_full['n_words']\n",
    "X_full['per_punct']=X_full['n_punct']*100/X_full['n_words']\n",
    "X_full['per_upper']=X_full['n_upper']*100/(X_full['n_letters']-X_full['n_punct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamos el texto para calcular la ratio de palabras cercanas a la BLACK_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full['comment_text'] = X_full[\"comment_text\"].apply(lambda x:process_text(x)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limpiamos el BLACK_LIST\n",
    "Definimos una nueva columna que nos medirá la ratio de palabras cercanas al BLACK_LIST. En principio era idea establecer una medida de ratio de palabras del blacklist en cada comentario, pero por performance no he sido capaz de lograr ponerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aquí es donde se queda pillado\n",
    "#X_full['d_badwords']= stds.dist_series_to_series(X_full[\"comment_text\"],BLACK_LIST).sum()/ X_full['n_words']\n",
    "#BLACK_LIST\n",
    "#bl = set(BLACK_LIST)\n",
    "#len(bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = TfidfVectorizer(min_df=100,  max_features=30000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "count_vectorizer.fit(X_full['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vec =  count_vectorizer.transform(X_full['comment_text'].iloc[:X_train.shape[0]]).toarray()\n",
    "test_vec = count_vectorizer.transform(X_full['comment_text'].iloc[X_train.shape[0]:]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos todas las características que queremos, volvemos a separar el dataset en el de entrenamiento y el de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_train = X_full.iloc[0:len(X_train),]\n",
    "custom_test  = X_full.iloc[len(X_train):,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos un CAP de 200 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom_train['n_unique_word'].loc[custom_train['n_unique_word']>200] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTED_COLS=['n_sentences','n_words','n_unique_word','n_letters'\n",
    "               ,'n_punct','n_upper','n_title','n_stopwords'\n",
    "               ,'mean_word_len','per_wordunique','per_punct']#,'d_badwords']\n",
    "\n",
    "target_x = custom_train[SELECTED_COLS]\n",
    "test_x   = custom_test[SELECTED_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(train_vec.shape[1]+len(SELECTED_COLS),))\n",
    "x = Dense(256, init='normal', activation=\"relu\")(inp)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128, init='normal', activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(64, init='normal', activation='relu')(x)\n",
    "x = Dense(32, init='normal', activation='relu')(x)\n",
    "x = Dense(6, init='normal', activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sparse = scipy.sparse.hstack([target_x,train_vec])\n",
    "test_sparse  = scipy.sparse.hstack([test_x,test_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_sparse.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algunos parámetros para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "val_prop = 0.2\n",
    "es_patience = 5\n",
    "rlr_patience = 2\n",
    "rlr_cooldown = 4\n",
    "valid_split = 0.2\n",
    "file_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_roc_auc', patience=es_patience, mode='max',  verbose=0)\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_roc_auc', verbose=0, mode='max',   save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau( monitor='val_roc_auc', \n",
    "                              factor=0.5, \n",
    "                              patience=rlr_patience, \n",
    "                              cooldown=rlr_cooldown, \n",
    "                              min_lr=1e-4)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train, Y_train, validation_split=valid_split, nb_epoch=epochs, batch_size=batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunos Resultados\n",
    "\n",
    "##### validation_split=0.2, nb_epoch=2 --> \n",
    "    - 76s 593us/step - loss: 0.0717 - acc: 0.9766 - val_loss: 0.0633 - val_acc: 0.9776\n",
    "    - 75s 586us/step - loss: 0.0649 - acc: 0.9782 - val_loss: 0.0720 - val_acc: 0.9765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test_sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission[ct.OBJECTIVE_COLS] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
